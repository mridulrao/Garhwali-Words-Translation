{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0e2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385dbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f578f4",
   "metadata": {},
   "source": [
    "Step1 - Find all the possible translation from google\n",
    "\n",
    "keys to classify into -\n",
    "\n",
    "n = Noun\n",
    "v = Verb\n",
    "a = Adjective\n",
    "s = Adjective Satellite\n",
    "r = Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0043798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#translation_list = ['disabled', 'handicapped', 'anamorphous', 'maimed', 'cripple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286f22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_candidate_set(candidate_dict_synsets, synset, pos):\n",
    "    if pos not in candidate_dict_synsets.keys():\n",
    "        candidate_dict_synsets.update( {pos : [synset]} )\n",
    "    elif pos in candidate_dict_synsets.keys():\n",
    "        values = candidate_dict_synsets[pos]\n",
    "        if synset not in values:                       \n",
    "            candidate_dict_synsets[pos].append(synset)\n",
    "    else:\n",
    "        print(\"pos error\")\n",
    "                \n",
    "    return candidate_dict_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a131532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_set(translation_list):\n",
    "    candidate_dict_synsets = {}\n",
    "    for translation in translation_list:\n",
    "        synsets = wordnet.synsets(translation)\n",
    "        \n",
    "        for synset in synsets:\n",
    "            match = re.search(r\"(?<=Synset\\(')[^']+\", str(synset))\n",
    "            _synset = match.group(0)\n",
    "            pos = match.group(0).split(\".\")[1]\n",
    "            \n",
    "            candidate_dict_synsets = add_to_candidate_set(candidate_dict_synsets=candidate_dict_synsets,\n",
    "                                                                  synset=_synset,\n",
    "                                                                  pos=pos)\n",
    "            \n",
    "    return candidate_dict_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48814d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#candidate_set = get_candidate_set(translation_list)\n",
    "#print(candidate_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f444c9",
   "metadata": {},
   "source": [
    "## Syntactic Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7eb8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntacticVector:\n",
    "    def __init__(self):\n",
    "        self.ner_tagger = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz', 'stanford-ner-4.2.0.jar')\n",
    "        \n",
    "    def calculate_rank(self, possible_sense, sense):\n",
    "        word = possible_sense.split('.')[0]\n",
    "        all_synsets = wordnet.synsets(word)\n",
    "        rank = 0\n",
    "        for _synset in all_synsets:\n",
    "            match = re.search(r\"(?<=Synset\\(')[^']+\", str(_synset))\n",
    "            if match.group(0) == sense:\n",
    "                return (rank+1)\n",
    "            rank += 1\n",
    "\n",
    "        return rank\n",
    "\n",
    "    \n",
    "    def get_sync(self, word_pos, sense, candidate_set):\n",
    "        pos_list = list(candidate_set.keys())\n",
    "        if word_pos in pos_list:\n",
    "            if sense in candidate_set[word_pos]:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def get_synn(self, word):\n",
    "        tokens = word_tokenize(word)\n",
    "        tags = self.ner_tagger.tag(tokens)\n",
    "\n",
    "        named_entities = [tag for word, tag in tags if tag != 'O']\n",
    "        if named_entities:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_synr(self, word, sense, candidate_set, word_pos, sync):\n",
    "        pos_matched_candidate = list(candidate_set[word_pos])\n",
    "\n",
    "        synr = 0\n",
    "        rank = 1\n",
    "        for possible_sense in pos_matched_candidate:\n",
    "            rank = self.calculate_rank(possible_sense, sense)\n",
    "            try:\n",
    "                synr += 1/rank\n",
    "            except ZeroDivisionError:\n",
    "                print(\"Zero Division\")\n",
    "                continue\n",
    "\n",
    "        return sync*synr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29bac294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_syntactic_vector(word, word_pos, sense, candidate_set):    \n",
    "    vector = []\n",
    "    syntactic_vector = SyntacticVector()\n",
    "    vector.append(syntactic_vector.get_sync(word_pos, sense, candidate_set))\n",
    "    vector.append(syntactic_vector.get_synn(word))\n",
    "    vector.append(syntactic_vector.get_synr(word, sense, candidate_set, word_pos, vector[0]))\n",
    "    \n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c41449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word = 'handicap'\n",
    "#word_pos = 'v'\n",
    "#possible_sense = candidate_set[word_pos][2]\n",
    "\n",
    "#generate_syntactic_vector(word, word_pos, possible_sense, candidate_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c4783",
   "metadata": {},
   "source": [
    "## Semantic Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45fc74b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticVector():\n",
    "    def get_semc(self, sense_1, sense_2):\n",
    "        semc1 = sense_1.split(\".\")[1]\n",
    "        semc2 = sense_2.split(\".\")[1]\n",
    "\n",
    "        if semc1 == semc2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_semr(self, sense_1, sense_2):\n",
    "        synset_1 = wordnet.synset(sense_1)\n",
    "        synset_2 = wordnet.synset(sense_2)\n",
    "\n",
    "        if synset_1 == synset_2:\n",
    "            return 1\n",
    "\n",
    "        if synset_1 in synset_2.hypernyms() or synset_1 in synset_2.hyponyms() or synset_2 in synset_1.hypernyms() or synset_2 in synset_1.hyponyms():\n",
    "            return 0.8\n",
    "\n",
    "        if synset_1.hypernyms() == synset_2.hypernyms():\n",
    "            return 0.7\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    def get_semd(self, sense_1, sense_2):\n",
    "        synset_1 = wordnet.synset(sense_1)\n",
    "        synset_2 = wordnet.synset(sense_2)\n",
    "\n",
    "        semd = synset_1.shortest_path_distance(synset_2)\n",
    "        \n",
    "        if semd == None:\n",
    "            return 0\n",
    "\n",
    "        return float(semd)\n",
    "    \n",
    "    def get_sems(self, sense_1, sense_2):\n",
    "        s1 = wordnet.synset(sense_1)\n",
    "        s2 = wordnet.synset(sense_2)\n",
    "\n",
    "        gloss1 = s1.definition()\n",
    "        gloss2 = s2.definition()\n",
    "\n",
    "        tokens1 = set(word_tokenize(gloss1))\n",
    "        tokens2 = set(word_tokenize(gloss2))\n",
    "\n",
    "        overlap = tokens1.intersection(tokens2)\n",
    "\n",
    "        sems = len(overlap) / (len(tokens1) + len(tokens2) - len(overlap))\n",
    "\n",
    "        return float(sems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176800e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_vector(sense_1, sense_2):\n",
    "    semantic_vector = SemanticVector()\n",
    "    vector = []\n",
    "    vector.append(semantic_vector.get_semc(sense_1, sense_2))\n",
    "    vector.append(semantic_vector.get_semr(sense_1, sense_2))\n",
    "    vector.append(semantic_vector.get_semd(sense_1, sense_2))\n",
    "    vector.append(semantic_vector.get_sems(sense_1, sense_2))\n",
    "    \n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc60527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sense_1 = 'disabled.n.01'\n",
    "#sense_2 = 'disabled.s.01'\n",
    "\n",
    "#generate_semantic_vector(sense_1, sense_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a310a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value_list = []\n",
    "\n",
    "#for value in values:\n",
    "    #value_list += value\n",
    "    \n",
    "#print(value_list)\n",
    "\n",
    "#sense_1 = 'disabled.n.01'\n",
    "#for sense_2 in value_list:\n",
    "    #if sense_2 != sense_1:\n",
    "        #print(sense_2)\n",
    "        #print(generate_semantic_vector(sense_1, sense_2))\n",
    "    #else:\n",
    "        #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb78805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(word, word_pos, sense_1, candidate_set, flag):\n",
    "    feature_vectors = []\n",
    "    y_label = []\n",
    "    syntactic_vector = generate_syntactic_vector(word=word, word_pos=word_pos, \n",
    "                                                 sense=sense_1, candidate_set=candidate_set)\n",
    "    \n",
    "    #get list of all senses\n",
    "    value_list = []\n",
    "    values = candidate_set.values()\n",
    "    for value in values:\n",
    "        value_list += value\n",
    "        \n",
    "    #generate semantic vector for each sense_1, sense_2 pair \n",
    "    for sense_2 in value_list:\n",
    "        symantic_vector = generate_semantic_vector(sense_1=sense_1, sense_2=sense_2)\n",
    "        feature_vector = np.concatenate((syntactic_vector, symantic_vector))\n",
    "        feature_vectors.append(feature_vector)\n",
    "        if flag:\n",
    "            y_label.append(1)\n",
    "        else:\n",
    "            y_label.append(0)\n",
    "            \n",
    "    return feature_vectors, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56cffabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word = 'handicap'\n",
    "#word_pos = 'n'\n",
    "#possible_sense = candidate_set[word_pos][2]\n",
    "#get_feature_vector(word, word_pos, possible_sense, candidate_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f60390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_sense(translation_list):\n",
    "    candidate_set = get_candidate_set(translation_list)\n",
    "    values = []\n",
    "    for val in candidate_set.values():\n",
    "        values += val\n",
    "    \n",
    "    for i in range(len(values)):\n",
    "        print(str(i) + \" = \" + str(values[i]))\n",
    "    \n",
    "    index = int(input(\"Enter the index corresponding to correct sense\"))\n",
    "    return values[index], candidate_set, values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa2054b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_correct_sense(translation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cdda7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation():\n",
    "    print(\"ENTER ! TO STOP TAKING TRANSLATIONS\")\n",
    "    word = input(\"Enter Word \")\n",
    "    word_pos = input(\"Enter word's pos \")\n",
    "\n",
    "    translation_list = []\n",
    "    \n",
    "    while True:\n",
    "        key = input(\"Enter translation \")\n",
    "        if key == \"!\":\n",
    "            break\n",
    "        else:\n",
    "            translation_list.append(key)\n",
    "        \n",
    "    return translation_list, word, word_pos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "763bd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_translation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4547db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datapoint():\n",
    "    df = pd.DataFrame(columns = [\"Word\", \"Word POS\", \"Sense\", \"Feature Vector\", \"Label\"])\n",
    "    translation_list, word, word_pos = get_translation()\n",
    "    correct_sense, candidate_set, candidate_list = get_correct_sense(translation_list)\n",
    "    \n",
    "    for possible_sense in candidate_list:\n",
    "        if possible_sense == correct_sense:\n",
    "            feature_vectors, y_label = get_feature_vector(word=word, word_pos=word_pos, \n",
    "                                                          sense_1 = possible_sense, \n",
    "                                                          candidate_set = candidate_set, flag = True)\n",
    "        else:\n",
    "            feature_vectors, y_label = get_feature_vector(word=word, word_pos=word_pos, \n",
    "                                                          sense_1 = possible_sense, \n",
    "                                                          candidate_set = candidate_set, flag = False)\n",
    "        \n",
    "        for i in range(len(feature_vectors)):\n",
    "            row = pd.Series({'Word' : word, \n",
    "                             'Word POS' : word_pos,\n",
    "                             'Sense' : possible_sense,\n",
    "                             'Feature Vector' : feature_vectors[i],\n",
    "                             'Label' : y_label[i]})   \n",
    "            df = pd.concat([df, row.to_frame().T], ignore_index = True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe8ea6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_datapoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4978a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_datapoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f9b3bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTER ! TO STOP TAKING TRANSLATIONS\n",
      "Enter Word ज्योतिषी\n",
      "Enter word's pos n\n",
      "Enter translation astrologer\n",
      "Enter translation !\n",
      "0 = astrologer.n.01\n",
      "Enter the index corresponding to correct sense0\n",
      "Type STOP to generate dataframe else type anythingn\n",
      "ENTER ! TO STOP TAKING TRANSLATIONS\n",
      "Enter Word फीका\n",
      "Enter word's pos a\n",
      "Enter translation tasteless\n",
      "Enter translation faded\n",
      "Enter translation !\n",
      "0 = tasteless.a.01\n",
      "1 = tasteless.a.02\n",
      "2 = fade.v.01\n",
      "3 = fade.v.02\n",
      "4 = evanesce.v.01\n",
      "5 = languish.v.03\n",
      "6 = bleached.s.01\n",
      "7 = attenuate.s.01\n",
      "Enter the index corresponding to correct sense0\n",
      "Type STOP to generate dataframe else type anythingSTOP\n",
      "[       Word Word POS            Sense                       Feature Vector  \\\n",
      "0  ज्योतिषी        n  astrologer.n.01  [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]   \n",
      "\n",
      "  Label  \n",
      "0     1  ,     Word Word POS           Sense  \\\n",
      "0   फीका        a  tasteless.a.01   \n",
      "1   फीका        a  tasteless.a.01   \n",
      "2   फीका        a  tasteless.a.01   \n",
      "3   फीका        a  tasteless.a.01   \n",
      "4   फीका        a  tasteless.a.01   \n",
      "..   ...      ...             ...   \n",
      "59  फीका        a  attenuate.s.01   \n",
      "60  फीका        a  attenuate.s.01   \n",
      "61  फीका        a  attenuate.s.01   \n",
      "62  फीका        a  attenuate.s.01   \n",
      "63  फीका        a  attenuate.s.01   \n",
      "\n",
      "                                       Feature Vector Label  \n",
      "0                 [1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 1.0]     1  \n",
      "1   [1.0, 0.0, 2.0, 1.0, 0.7, 0.0, 0.1666666666666...     1  \n",
      "2                 [1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0]     1  \n",
      "3                 [1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0]     1  \n",
      "4                 [1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0]     1  \n",
      "..                                                ...   ...  \n",
      "59                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]     0  \n",
      "60                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]     0  \n",
      "61                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]     0  \n",
      "62                [0.0, 0.0, 0.0, 1.0, 0.7, 0.0, 0.0]     0  \n",
      "63                [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]     0  \n",
      "\n",
      "[64 rows x 5 columns]]\n",
      "        Word Word POS            Sense  \\\n",
      "0   ज्योतिषी        n  astrologer.n.01   \n",
      "0       फीका        a   tasteless.a.01   \n",
      "1       फीका        a   tasteless.a.01   \n",
      "2       फीका        a   tasteless.a.01   \n",
      "3       फीका        a   tasteless.a.01   \n",
      "..       ...      ...              ...   \n",
      "59      फीका        a   attenuate.s.01   \n",
      "60      फीका        a   attenuate.s.01   \n",
      "61      फीका        a   attenuate.s.01   \n",
      "62      फीका        a   attenuate.s.01   \n",
      "63      फीका        a   attenuate.s.01   \n",
      "\n",
      "                                       Feature Vector Label  \n",
      "0                 [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]     1  \n",
      "0                 [1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 1.0]     1  \n",
      "1   [1.0, 0.0, 2.0, 1.0, 0.7, 0.0, 0.1666666666666...     1  \n",
      "2                 [1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0]     1  \n",
      "3                 [1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0]     1  \n",
      "..                                                ...   ...  \n",
      "59                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]     0  \n",
      "60                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]     0  \n",
      "61                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]     0  \n",
      "62                [0.0, 0.0, 0.0, 1.0, 0.7, 0.0, 0.0]     0  \n",
      "63                [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]     0  \n",
      "\n",
      "[65 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "start = True \n",
    "frames = []\n",
    "\n",
    "while start:\n",
    "    df = generate_datapoint()\n",
    "    frames.append(df)\n",
    "    check = input(\"Type STOP to generate dataframe else type anything\")\n",
    "    if check == 'STOP':\n",
    "        start = False\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(frames)\n",
    "result = pd.concat(frames)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7de1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"Data_Set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f157ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
